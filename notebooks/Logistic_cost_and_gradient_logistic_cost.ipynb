{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La régression logistique\n",
    "\n",
    "- Méthode introduite par les statisticiens vers la fin des années 60 et popularisée par Andersen (1982).\n",
    "- Permet de s'affranchir des hypothèses restrictives associées aux méthode linéaires paramétriques.\n",
    "- Hypothèse:\n",
    "    - Logarithme des rapports de probabilités conditionnelles des classes pour une entrée $x$ est linéaire par rapport à $x$.\n",
    "\n",
    "$$\\ln{\\Big(\\frac{\\mathbb{P}(X =x | Y = 1)}{\\mathbb{P}(X = x | Y = -1)}\\Big)} = w_0 + \\langle \\bar{w}, x \\rangle$$\n",
    "\n",
    "\n",
    "- La probabilité à posteriori:\n",
    "\n",
    "$$\\mathbb{P}(Y = 1 | X = x) = \\frac{1}{1 + e^{-(\\tilde{w} + \\langle \\bar{w}, x \\rangle)}}$$\n",
    "\n",
    "- On utilise cette transformation dans le cas où y prend un valeur binaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de la fonction sigmoid ou logistique\n",
    "# N.B Dans le cas particulier de la régression logistique x\n",
    "# correspond à la combinaison linéaire de w et x (cf plus haut)\n",
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de coût\n",
    "\n",
    "\n",
    "$$\\hat{\\mathcal{L}}(\\textbf{w}) = \\frac{1}{m}\\sum_{t=1}^{m}\\ln{(1 + e^{-y_ih_w(x_i)})}$$\n",
    "\n",
    "Selon que la valeur de $y \\in \\left\\{-1, 1\\right\\}$ ou que $y \\in \\left\\{0, 1\\right\\}$\n",
    "\n",
    "### Pour $y \\in \\left\\{0, 1\\right\\}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(y=1 | x; \\theta)  = h_\\theta(x)\n",
    "$$\n",
    "$$ \\mathbb{P}(y=0 | x; \\theta)  = 1 - h_\\theta(x) $$\n",
    "\n",
    "Ainsi:\n",
    "\n",
    "\n",
    "$$ p(y | x; \\theta) = (h_\\theta(x))^y (1 - h_\\theta(x)^{1-y}$$\n",
    "\n",
    "L'équation ci-dessus correspond à une modélisation de Bernoulli. Partant du principe que les observations sont indépendantes. La fonction de vraisemblance correspond à:\n",
    "\n",
    "$$ \\mathcal{L} = \\prod_{i = 1}^{m} p(y^{(i)} | x^{(i)}; \\theta)$$\n",
    "ce qui correspond à:\n",
    "\n",
    "$$ \\mathcal{L} = \\prod_{i = 1}^{m}(h_\\theta(x))^{y^{(i)}} (1 - h_\\theta(x))^{{1-y}^{(i)}}$$\n",
    "\n",
    "Formule pour la maximisation de la log-vraisemblance:\n",
    "\n",
    "$$ \\log(\\mathcal{L}) = \\sum_{i=1}^{m} y^{(i)} \\log h(x^{(i)}) + (1 - y^{(i)}) \\log(1 - h(x^{(i)})$$\n",
    "\n",
    "Il existe toutefois une seconde notation possible dans le cas où $y \\in \\left\\{-1, 1 \\right\\}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iaamini/Documents/ML_practice/ML_fundamentals/virtualenv/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4d7d9652b0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x= np.linspace(-1000, 1000, 101)\n",
    "plt.plot(x, logistic(x))\n",
    "plt.plot(x, stable_logistic(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le gradient de la fonction:\n",
    "\n",
    "$$ \\nabla \\hat{\\mathcal{L}}(\\textbf{w}) = \\frac{1}{m}\\sum_{t=1}^{m}y_i \\Big(1 - \\frac{1}{1 + e^{-y_ih_w(x_i)}}\\Big) \\times x_i $$\n",
    "\n",
    "- Pour l'apprentissage des paramètres du modèle de la régression logistique en utilisant la méthode du gradient conjugué  pour minimser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_log_s_loss(w, X, y):\n",
    "    # defining dim variables\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # initiating g: gradient vector\n",
    "    g = np.zeros(d+1)\n",
    "    \n",
    "    # Computing dot product\n",
    "    ps = np.dot(X, w[1:]) + w[0]\n",
    "#     print((1-sigmoid(y*ps)).shape)\n",
    "#     print(X.shape)\n",
    "#     print(np.dot(1-sigmoid(y*ps), X).shape)\n",
    "#     print(np.dot(1-sigmoid(y*ps), X).shape)\n",
    "    g[1:] = np.dot(1-logistic(y*ps), X)\n",
    "    g[0] = (1-(logistic(y*ps)) * y).sum()\n",
    "#     print(g[0])\n",
    "    g /= n\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(-10., 10, 1)\n",
    "# bias = np.ones(len(X))\n",
    "# X = np.vstack([X,bias]) # Add intercept\n",
    "w = np.random.random(2)\n",
    "\n",
    "y = np.dot(X[:, None], w[1:]) + w[0]\n",
    "y = logistic(y)\n",
    "y = np.random.binomial(1., y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_surrogate_loss(w, X, y):\n",
    "    # Computing the dot product\n",
    "    n, d = X.shape\n",
    "    ps = np.dot(X, w[1:]) + w[0]\n",
    "    loss = logistic(y*ps).sum() / n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6990452965966952"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_surrogate_loss(w, X[:, np.newaxis], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,\n",
       "         1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
