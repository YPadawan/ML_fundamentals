{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import logistic_regression as lg\n",
    "import optimizers as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=10, n_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random(X.shape[1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.1633637010224795"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X, w[:X.shape[1]]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La régression logistique\n",
    "\n",
    "- Méthode introduite par les statisticiens vers la fin des années 60 et popularisée par Andersen (1982).\n",
    "- Permet de s'affranchir des hypothèses restrictives associées aux méthode linéaires paramétriques.\n",
    "- Hypothèse:\n",
    "    - Logarithme des rapports de probabilités conditionnelles des classes pour une entrée $x$ est linéaire par rapport à $x$.\n",
    "\n",
    "$$\\ln{\\Big(\\frac{\\mathbb{P}(X =x | Y = 1)}{\\mathbb{P}(X = x | Y = -1)}\\Big)} = w_0 + \\langle \\bar{w}, x \\rangle$$\n",
    "\n",
    "\n",
    "- La probabilité à posteriori:\n",
    "\n",
    "$$\\mathbb{P}(Y = 1 | X = x) = \\frac{1}{1 + e^{-(\\tilde{w} + \\langle \\bar{w}, x \\rangle)}}$$\n",
    "\n",
    "Ci-dessous se trouve la fonction permettant de calculer l'équation ci-dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lien avec le principe MRE\n",
    "## Gradient de la fonction de coût\n",
    "\n",
    "### Fonction de coût:\n",
    "\n",
    "$$ \\hat{\\mathcal{L}}(\\textbf{w}) = \\frac{1}{m}\\sum_{t=1}^{m}\\ln{(1 + e^{-y_ih_w(x_i)})}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def log_surr_loss(w, X, y):\n",
    "    # Computing the dot product\n",
    "    n, d = X.shape\n",
    "    ps = np.dot(X, w[1:]) + w[0]\n",
    "    total = sigmoid(y*ps).sum() / n\n",
    "    return total\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5202844501222914"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_surr_loss(w, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le gradient de la fonction:\n",
    "\n",
    "$$ \\nabla \\hat{\\mathcal{L}}(\\textbf{w}) = \\frac{1}{m}\\sum_{t=1}^{m}y_i \\Big(1 - \\frac{1}{1 + e^{-y_ih_w(x_i)}}\\Big) \\times x_i $$\n",
    "\n",
    "- Pour l'apprentissage des paramètres du modèle de la régression logistique en utilisant la méthode du gradient conjugué  pour minimser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.72971555, -0.0453312 , -0.08847367, -0.09727155, -0.29370875,\n",
       "       -0.15890609])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_log_s_loss(w, X, y):\n",
    "    # defining dim variables\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # initiating g: gradient vector\n",
    "    g = np.zeros(d+1)\n",
    "    \n",
    "    # Computing dot product\n",
    "    ps = np.dot(X, w[1:]) + w[0]\n",
    "#     print((1-sigmoid(y*ps)).shape)\n",
    "#     print(X.shape)\n",
    "#     print(np.dot(1-sigmoid(y*ps), X).shape)\n",
    "#     print(np.dot(1-sigmoid(y*ps), X).shape)\n",
    "    g[1:] = np.dot(1-sigmoid(y*ps), X)\n",
    "    g[0] = (1-(sigmoid(y*ps)) * y).sum()\n",
    "#     print(g[0])\n",
    "    g /= n\n",
    "    return g\n",
    "\n",
    "gradient_log_s_loss(w, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_test = w\n",
    "niter = []\n",
    "losses = []\n",
    "for i in range(10000):\n",
    "    w_test = w_test - 0.01 * gradient_log_s_loss(w_test, X, y)\n",
    "    niter.append(i)\n",
    "    losses.append(log_surr_loss(w_test, X, y))\n",
    "#     print(w_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f828f350358>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(niter, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33396354,  0.32284559, -0.14552576, -0.27102596,  0.29711763,\n",
       "        0.53350802])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.gradient(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Activation function used to map any real value between 0 and 1\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def net_input(w, X):\n",
    "    # Computes the weighted sum of inputs\n",
    "    return np.dot(X, w[1:]) + w[0]\n",
    "\n",
    "def probability(w, X):\n",
    "    # Returns the probability after passing through sigmoid\n",
    "    return sigmoid(net_input(w, X))\n",
    "\n",
    "def cost_function(w, X, y):\n",
    "    # Computes the cost function for all the training samples\n",
    "    m = X.shape[0]\n",
    "    total_cost = -(1 / m) * np.sum(\n",
    "        y * np.log(probability(w, X)) + (1 - y) * np.log(\n",
    "            1 - probability(w, X)))\n",
    "    return total_cost\n",
    "\n",
    "def gradient(w, X, y):\n",
    "    # Computes the gradient of the cost function at the point theta\n",
    "    m = X.shape[0]\n",
    "    return (1 / m) * np.dot(X.T, sigmoid(net_input(w,   X)) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.45006556, -0.41263723, -0.05360261,  0.29582062,  0.10287432])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(w, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e7a9d8d50110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# w = np.random.random(d)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_logistic_surrogate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/logistic_regression.py\u001b[0m in \u001b[0;36mgradient_logistic_surrogate_loss\u001b[0;34m(w, X, y)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mVecteur\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0mde\u001b[0m \u001b[0mla\u001b[0m \u001b[0mfonction\u001b[0m \u001b[0mde\u001b[0m \u001b[0mcoût\u001b[0m \u001b[0mlogistique\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "w0 = 2\n",
    "# w = np.random.random(d)\n",
    "lg.gradient_logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.71449067e-05,  2.48636945e-05,  4.83234818e-05, -1.36803929e-05,\n",
       "       -4.16710138e-05, -7.50935571e-05, -6.45455393e-04, -1.08207211e-04,\n",
       "       -3.80907106e-05,  2.04690505e-04, -6.56245390e-04])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = 2\n",
    "# w = np.random.random(d)\n",
    "lg.gradient_logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random(X.shape[1]+1)\n",
    "Lold = lg.logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7499999954830452"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc834eda5f8>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test = np.linspace(-100, 100, 101)\n",
    "plt.plot(x_test, lg.logistic(x_test))\n",
    "plt.plot(x_test, lg.stable_logistic(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1/(1+np.exp(-100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage des paramètres du modèle\n",
    "\n",
    "La régression linéaire utilise la méthode du gradient conjugué afin de minimiser la fonction de coût.\n",
    "\n",
    "Le gradient conjugué utilise lui-même une recherche linéaire afin de trouver le pas d'apprentissage optimal.\n",
    "Un brève présentation de l'algorithme de recherche linéaire s'impose donc.\n",
    "\n",
    "## recherche linéaire\n",
    "\n",
    "On cherche une valeur maximale admissible du pas d'apprentissage en allant selon un direction de descente $p_t$ (vérifiant $p_t^T\\nabla\\mathcal{L}(w)$.\n",
    "On suit toujours un règle de mise à jour avec la condition de décroissance de la fonction de coût.\n",
    "$$\\forall{t} \\in \\mathbb{N}, \\hat{\\mathcal{L}}(w^{(t+1)}) < \\hat{\\mathcal{L}}(w^{(t)}) $$\n",
    "\n",
    "L'algorithme de recherche linéaire consiste à vérifier des conditions que l'on appelle *conditions de Wolfe*.\n",
    "\n",
    "### Condition d'Armijo\n",
    "\n",
    "Permet de répondre aux deux situations où l'équation ci-dessus peut être satisfaite sans pour autant atteindre le minimiseur de $\\mathcal{L}$.\n",
    "\n",
    "$$\\forall{t} \\in \\mathbb{N}, \\hat{\\mathcal{L}}(w^{(t)} + \\eta_tp_t) \\leq \\hat{\\mathcal{L}}(w_t) + \\alpha\\eta_tp_t^{T}\\nabla\\hat{\\mathcal{L}}(w_t)$$\n",
    "\n",
    "la contrainte de décroissance linéaire implique que le taux de décroissance allant $\\hat{\\mathcal{L}}(w^t)$ à $\\hat{\\mathcal{L}}(w^{t+1})$ ne doit pas être plus grand que la descente pondéré par un coefficient alpha.\n",
    "\n",
    "### Condition de courbure\n",
    "\n",
    "Cette condition implique que la descente lors de l'itération suivante soit au moins égale à une fraction $\\beta \\in (\\alpha, 1)$. Donc:\n",
    "$$ \\forall{t} \\in \\mathbb{N}, p^T_t\\nabla\\hat{\\mathcal{L}}(w^{(t)} + \\eta_tp_t) \\geq \\beta p^T_t\\nabla\\hat{\\mathcal{L}}(w^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.arange(10)\n",
    "p = q + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[1] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lg.gradient_logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.2\n",
    "eta2 = 1\n",
    "a = np.array([[1/eta**2, 1/eta2**2], [-eta2/eta, eta/eta2]])\n",
    "z = np.array([1-2-6, 3-4-1]).reshape(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7, -2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-176.99999999999997"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a @ z).reshape(1, -1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = 1/(eta-eta2) * np.dot(a,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = ab.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = ab[0][0], ab[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.random(d+1)\n",
    "w2 = np.random.random(d+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11212352, 0.11421168, 0.69384272, 0.14726285, 0.26871746,\n",
       "       0.33566604, 0.80316115, 0.45338342, 0.51985261, 0.39715825,\n",
       "       0.67430127])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7498172908812524"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.logistic_surrogate_loss(X, y, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = lg.logistic_surrogate_loss(X, y, w2)\n",
    "g = lg.gradient_logistic_surrogate_loss(X, y, w2)\n",
    "p = -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(36)line_search()\n",
      "-> alpha = 1e-4\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(37)line_search()\n",
      "-> min_eta = 1e-7\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(39)line_search()\n",
      "-> n, d = X.shape\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(42)line_search()\n",
      "-> pente = p @ g\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(45)line_search()\n",
      "-> _max = 0.\n",
      "(Pdb) print(pente)\n",
      "-0.0007134817264646589\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(47)line_search()\n",
      "-> if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(48)line_search()\n",
      "-> _max = np.abs(p[j]) / np.maximum(np.abs(w_old[j]), 1.)\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) _max\n",
      "0.002426252334694755\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(47)line_search()\n",
      "-> if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(47)line_search()\n",
      "-> if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(47)line_search()\n",
      "-> if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(47)line_search()\n",
      "-> if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(47)line_search()\n",
      "-> if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(47)line_search()\n",
      "-> if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(48)line_search()\n",
      "-> _max = np.abs(p[j]) / np.maximum(np.abs(w_old[j]), 1.)\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(47)line_search()\n",
      "-> if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(47)line_search()\n",
      "-> if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(47)line_search()\n",
      "-> if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(47)line_search()\n",
      "-> if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(48)line_search()\n",
      "-> _max = np.abs(p[j]) / np.maximum(np.abs(w_old[j]), 1.)\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(46)line_search()\n",
      "-> for j in range(d+1):\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(49)line_search()\n",
      "-> eta_min = min_eta / _max\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(52)line_search()\n",
      "-> eta = 1.\n",
      "(Pdb) print(eta_min)\n",
      "5.473180577160928e-06\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(55)line_search()\n",
      "-> w = w_old + eta * p\n",
      "(Pdb) print(w_old)\n",
      "[0.11212352 0.11421168 0.69384272 0.14726285 0.26871746 0.33566604\n",
      " 0.80316115 0.45338342 0.51985261 0.39715825 0.67430127]\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(57)line_search()\n",
      "-> new_loss = cost_func(X, y, w)\n",
      "(Pdb) print(w)\n",
      "[0.11454977 0.11351943 0.69249732 0.14764374 0.26987764 0.33775676\n",
      " 0.82113165 0.45639608 0.52091312 0.39145934 0.69257218]\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(60)line_search()\n",
      "-> while new_loss > (old_loss + alpha * eta * pente):\n",
      "(Pdb) print(new_loss)\n",
      "0.7498344079926408\n",
      "(Pdb) print(old_loss)\n",
      "0.7498172908812524\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-fb6817f57490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m w2, test_new_loss = opt.line_search(X, y, cost_func=lg.logistic_surrogate_loss, p=p, \n\u001b[0;32m----> 2\u001b[0;31m                                     g=g, old_loss=loss, w_old=w2)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# w2, test_new_loss = line_search(X, y, w2, cost_func=logistic_surrogate_loss, p=p, g=g)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py\u001b[0m in \u001b[0;36mline_search\u001b[0;34m(X, y, cost_func, p, g, old_loss, w_old)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Boucler tant que la condition d'Armijo n'est pas complète (Eq. 2.18)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mnew_loss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mold_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpente\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0meta_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py\u001b[0m in \u001b[0;36mline_search\u001b[0;34m(X, y, cost_func, p, g, old_loss, w_old)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Boucler tant que la condition d'Armijo n'est pas complète (Eq. 2.18)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0mnew_loss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mold_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpente\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0meta_min\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w2, test_new_loss = opt.line_search(X, y, cost_func=lg.logistic_surrogate_loss, p=p, \n",
    "                                    g=g, old_loss=loss, w_old=w2)\n",
    "# w2, test_new_loss = line_search(X, y, w2, cost_func=logistic_surrogate_loss, p=p, g=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.77084941, 0.51170377, 0.48691143, 0.59902129, 0.99471183,\n",
       "       0.61206021, 0.38004313, 0.72647739, 0.3834334 , 0.98672469,\n",
       "       0.66083577])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7499999999645529"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.logistic_surrogate_loss(X, y, w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6995819789587178"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.logistic_surrogate_loss(X, y, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(11)\n",
    "b = np.arange(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4270fb359cf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ps' is not defined"
     ]
    }
   ],
   "source": [
    "print(ps.T.shape, g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps @ g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-49.99999999999999"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient conjugué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(130)conjugate_gradient()\n",
      "-> epoque = 0\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(131)conjugate_gradient()\n",
      "-> n, d = X.shape\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(132)conjugate_gradient()\n",
      "-> w_old = np.random.random(d+1)\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(135)conjugate_gradient()\n",
      "-> new_loss = cost_func(X, y, w)\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(136)conjugate_gradient()\n",
      "-> old_loss = new_loss + 2 * epsilon\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(137)conjugate_gradient()\n",
      "-> g = grd_func(X, y, w)\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(139)conjugate_gradient()\n",
      "-> p = -g\n",
      "(Pdb) print(g)\n",
      "[ 31.43962682   0.06276278   5.54213954  -5.75087006   1.85309174\n",
      "  -9.14239035   3.75536922 -35.84533628  -6.12016493 -21.55472194\n",
      " -47.90183445]\n",
      "(Pdb) print(p)\n",
      "*** NameError: name 'p' is not defined\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(140)conjugate_gradient()\n",
      "-> print(new_loss)\n",
      "(Pdb) n\n",
      "0.2709816555412725\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(142)conjugate_gradient()\n",
      "-> while np.abs(old_loss - new_loss) > np.abs(old_loss) * epsilon:\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(143)conjugate_gradient()\n",
      "-> old_loss = new_loss\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(144)conjugate_gradient()\n",
      "-> w, new_loss = line_search(X=X, y=y, w=w, cost_func=cost_func,\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(145)conjugate_gradient()\n",
      "-> p=p, g=g, old_loss=old_loss,\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(146)conjugate_gradient()\n",
      "-> new_loss=new_loss, w_old=w_old)\n",
      "(Pdb) n\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(36)line_search()\n",
      "-> alpha = 1e-4\n",
      "(Pdb) c\n",
      "0.25\n",
      "Epoque 0 and loss is: 0.25\n",
      "[-0.66040672  0.83106175 -0.07548299  0.35270637  0.54640131  0.42384557\n",
      "  0.82167585  2.09351085  0.64029927  1.1854789   2.9087172 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/logistic_regression.py:35: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(x) / (1. + np.exp(-x)),\n",
      "/home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/logistic_regression.py:17: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "opt.conjugate_gradient(X=X, y=y, w=w, \n",
    "                   cost_func=lg.logistic_surrogate_loss, \n",
    "                   grd_func=lg.gradient_logistic_surrogate_loss, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 31.43962682,   0.06276278,   5.54213954,  -5.75087006,\n",
       "         1.85309174,  -9.14239035,   3.75536922, -35.84533628,\n",
       "        -6.12016493, -21.55472194, -47.90183445])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.gradient_logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-856545bcae88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mps\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ps' is not defined"
     ]
    }
   ],
   "source": [
    "ps**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test des algos sur un table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "breast_cancer = pd.read_csv('../../DB/breast-cancer-wisconsin.data', sep=',', header=None)\n",
    "breast_cancer.columns = ['col' + str(i) for i in range(len(breast_cancer.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer.drop('col6', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = breast_cancer.iloc[:, 1:-1]\n",
    "y = breast_cancer.iloc[:, -1]\n",
    "y[y==2] = -1\n",
    "y[y==4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, \n",
    "                                                    test_size=.4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a992a4006bbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "w = np.random.random(X.shape[1]+1)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(131)conjugate_gradient()\n",
      "-> epoque = 0\n",
      "(Pdb) c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/logistic_regression.py:35: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(x) / (1.0 + np.exp(-x)),\n",
      "/home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/logistic_regression.py:17: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34844868735083534\n",
      "> /home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/optimizers.py(36)line_search()\n",
      "-> alpha = 1e-4\n",
      "(Pdb) c\n",
      "[-421.13048483 -185.17699511 -199.66373247 -189.81458539 -290.04887726\n",
      " -278.79262835 -171.51222662 -145.942052   -135.70490661]\n",
      "[-177.77731091  -78.07850458  -84.19366181  -79.82934308 -122.32859962\n",
      " -117.71187978  -72.20796585  -61.64890042  -56.89658338]\n",
      "[-77.3517843  -33.88174257 -36.54216638 -34.44129369 -53.11480251\n",
      " -51.23804366 -31.22767978 -26.86330875 -24.37443775]\n",
      "[-33.35423416 -14.51864518 -15.66551151 -14.55627992 -22.79146132\n",
      " -22.11511012 -13.2737566  -11.6233507  -10.1261208 ]\n",
      "[-14.27734516  -6.12300364  -6.61360689  -5.93434077  -9.64356985\n",
      "  -9.48770316  -5.48911862  -5.01546032  -3.9481959 ]\n",
      "[-5.99081775 -2.47614519 -2.68168403 -2.18918425 -3.9324519  -4.00267078\n",
      " -2.10766496 -2.14515664 -1.26465855]\n",
      "[-2.39249087 -0.89253988 -0.97429287 -0.56289417 -1.45246624 -1.62086011\n",
      " -0.63930856 -0.89876109 -0.09936408]\n",
      "[-0.82987392 -0.20484014 -0.23283762  0.14334196 -0.37550248 -0.58653003\n",
      " -0.00165704 -0.35749875  0.40667913]\n",
      "[-0.15129513  0.09379893  0.0891452   0.4500306   0.09217756 -0.137364\n",
      "  0.27524818 -0.1224513   0.62643241]\n",
      "[ 0.14330576  0.22345128  0.2289321   0.58317762  0.29521803  0.05763873\n",
      "  0.39546491 -0.02040686  0.72183696]\n",
      "[0.27124769 0.27975787 0.28964002 0.64100191 0.38339628 0.14232627\n",
      " 0.44767372 0.02390992 0.76327012]\n",
      "[0.32680689 0.30420919 0.31600263 0.66611229 0.42168797 0.17910211\n",
      " 0.47034557 0.04315462 0.7812626 ]\n",
      "[0.350934   0.31482741 0.32745084 0.67701672 0.4383165  0.19507237\n",
      " 0.48019103 0.05151182 0.789076  ]\n",
      "[0.3614114  0.31943846 0.33242232 0.68175205 0.44553758 0.20200758\n",
      " 0.4844665  0.055141   0.79246904]\n",
      "[0.3659613  0.32144084 0.33458123 0.68380841 0.44867339 0.20501926\n",
      " 0.48632316 0.056717   0.7939425 ]\n",
      "[0.36793713 0.3223104  0.33551875 0.68470141 0.45003515 0.20632711\n",
      " 0.48712944 0.0574014  0.79458236]\n",
      "[0.36879515 0.32268801 0.33592588 0.6850892  0.4506265  0.20689505\n",
      " 0.48747957 0.0576986  0.79486022]\n",
      "[0.36916776 0.32285199 0.33610268 0.6852576  0.4508833  0.20714169\n",
      " 0.48763161 0.05782766 0.79498089]\n",
      "[0.36932956 0.3229232  0.33617946 0.68533073 0.45099482 0.20724879\n",
      " 0.48769764 0.05788371 0.79503329]\n",
      "[0.36939983 0.32295412 0.3362128  0.68536248 0.45104325 0.2072953\n",
      " 0.48772631 0.05790805 0.79505604]\n",
      "[0.36943034 0.32296755 0.33622728 0.68537627 0.45106428 0.2073155\n",
      " 0.48773877 0.05791862 0.79506592]\n",
      "[0.36944359 0.32297338 0.33623356 0.68538226 0.45107341 0.20732427\n",
      " 0.48774417 0.05792321 0.79507021]\n",
      "[0.36944935 0.32297592 0.33623629 0.68538486 0.45107738 0.20732808\n",
      " 0.48774652 0.0579252  0.79507208]\n",
      "[0.36945185 0.32297702 0.33623748 0.68538599 0.4510791  0.20732973\n",
      " 0.48774754 0.05792607 0.79507289]\n",
      "[0.36945293 0.32297749 0.33623799 0.68538648 0.45107985 0.20733045\n",
      " 0.48774798 0.05792644 0.79507324]\n",
      "[0.3694534  0.3229777  0.33623822 0.6853867  0.45108017 0.20733076\n",
      " 0.48774818 0.05792661 0.79507339]\n",
      "[0.36945361 0.32297779 0.33623832 0.68538679 0.45108031 0.2073309\n",
      " 0.48774826 0.05792668 0.79507346]\n",
      "[0.3694537  0.32297783 0.33623836 0.68538683 0.45108037 0.20733096\n",
      " 0.48774829 0.05792671 0.79507349]\n",
      "0.34844868735083534\n",
      "Epoque 0 and loss is: 0.34844868735083534\n",
      "[0.36945376 0.32297786 0.33623839 0.68538686 0.45108042 0.207331\n",
      " 0.48774832 0.05792673 0.79507351]\n"
     ]
    }
   ],
   "source": [
    "w = opt.conjugate_gradient(X=X_train, y=y_train, w=w, \n",
    "                   cost_func=lg.logistic_surrogate_loss, \n",
    "                   grd_func=lg.gradient_logistic_surrogate_loss, \n",
    "                           epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36945376, 0.32297786, 0.33623839, 0.68538686, 0.45108042,\n",
       "       0.207331  , 0.48774832, 0.05792673, 0.79507351])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
