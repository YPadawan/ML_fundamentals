{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recherche Linéaire\n",
    "\n",
    "\n",
    "On cherche une valeur maximale admissible du pas d'apprentissage en allant selon un direction de descente $p_t$ (vérifiant $p_t^T\\nabla\\mathcal{L}(w)$.\n",
    "On suit toujours un règle de mise à jour avec la condition de décroissance de la fonction de coût.\n",
    "$$\\forall{t} \\in \\mathbb{N}, \\hat{\\mathcal{L}}(w^{(t+1)}) < \\hat{\\mathcal{L}}(w^{(t)}) $$\n",
    "\n",
    "L'algorithme de recherche linéaire consiste à vérifier des conditions que l'on appelle *conditions de Wolfe*.\n",
    "\n",
    "### Condition d'Armijo\n",
    "\n",
    "Permet de répondre aux deux situations où l'équation ci-dessus peut être satisfaite sans pour autant atteindre le minimiseur de $\\mathcal{L}$.\n",
    "\n",
    "$$\\forall{t} \\in \\mathbb{N}, \\hat{\\mathcal{L}}(w^{(t)} + \\eta_tp_t) \\leq \\hat{\\mathcal{L}}(w_t) + \\alpha\\eta_tp_t^{T}\\nabla\\hat{\\mathcal{L}}(w_t)$$\n",
    "\n",
    "la contrainte de décroissance linéaire implique que le taux de décroissance allant $\\hat{\\mathcal{L}}(w^t)$ à $\\hat{\\mathcal{L}}(w^{t+1})$ ne doit pas être plus grand que la descente pondéré par un coefficient alpha.\n",
    "\n",
    "### Condition de courbure\n",
    "\n",
    "Cette condition implique que la descente lors de l'itération suivante soit au moins égale à une fraction $\\beta \\in (\\alpha, 1)$. Donc:\n",
    "$$ \\forall{t} \\in \\mathbb{N}, p^T_t\\nabla\\hat{\\mathcal{L}}(w^{(t)} + \\eta_tp_t) \\geq \\beta p^T_t\\nabla\\hat{\\mathcal{L}}(w^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "import logistic_regression as lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_armijo(w, f, fval0, pk, pente, args = (), alpha=1e-3, eta0=1, eta_min=0):\n",
    "    w_new = w + eta0*pk\n",
    "    fval_eta0 = f(w_new, X, y)\n",
    "    print(fval_eta0)\n",
    "    \n",
    "#     print(fval0 + alpha*eta0*pente)\n",
    "    \n",
    "    if fval_eta0 <= fval0 + alpha*eta0*pente:\n",
    "        print(\"returning eta0 at line 9\")\n",
    "        return eta0, fval_eta0\n",
    "    # Sinon on calcul le minimiseur de l'interpolant quadratic\n",
    "    eta1 = -(pente) * eta0**2 / 2.0 * (fval_eta0 - fval0 - pente * eta0)\n",
    "    w_new = w + eta1*pk\n",
    "    fval_eta1 = f(w_new, X, y)\n",
    "    \n",
    "    if fval_eta1 <= fval_eta0 + alpha*eta1*pente:\n",
    "        print(\"returning eta1 at line 16\")\n",
    "        return eta1, fval_eta1\n",
    "    \n",
    "    while eta1 > eta_min:\n",
    "        factor = 1 / eta1 - eta0\n",
    "        \n",
    "        coeff1 = fval_eta1 - fval0 - eta1 * pente\n",
    "        coeff2 = fval_eta0 - fval0 - eta0 * pente\n",
    "\n",
    "        a = (coeff1/(eta0**2) - coeff2/(eta1**2)) / (eta0 - eta1)\n",
    "        b = (-eta1*coeff1/eta0**2)+eta0*coeff2/(eta1**2)/(eta0-eta1)\n",
    "\n",
    "        # Calcul des coefficients du polynôme d'interpolation de degré 3 (Eq. 2.33)\n",
    "#         a, b = factor * np.dot(mat1, mat2).flatten()\n",
    "        if a != 0:\n",
    "            delta = b**2 - 3*a*pente\n",
    "            if delta >= 0:\n",
    "                eta2 = (-b + np.sqrt(delta)) / 3*a\n",
    "            else:\n",
    "                raise ValueError(\"ls_armijo:problème d'interpolation\")\n",
    "        else:\n",
    "            eta2 = eta1/2\n",
    "            \n",
    "        w_new= w + eta2*pk\n",
    "        fval_eta2 = f(w_new, X, y)\n",
    "        \n",
    "        if fval_eta2 <= fval_eta0 + alpha*eta2*pente:\n",
    "            print(\"returning eta2 at line 44\")\n",
    "            return eta2, fval_eta2\n",
    "        if (eta1 - eta2) > eta1 / 2.0:\n",
    "            eta2 = eta2 / 2.0\n",
    "        eta0 = eta1\n",
    "        eta1 = eta2\n",
    "        fval_eta0 = fval_eta1\n",
    "        fval_eta1 = fval_eta2\n",
    "        \n",
    "    return None, fval_eta1  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search(wk, f, pk, gfk, old_fval, args = (),  alpha=1e-4, eta0=1, eta_min=0):\n",
    "    \"\"\"Temporary docstring for the line search\n",
    "    X: array_like, matrix of data\n",
    "    y: array_like, true labels\n",
    "    w: array_like, weight vector\n",
    "    f: callable, cost function\n",
    "    pk: array_like, search direction\n",
    "    gfk: array_like, gradient vector\n",
    "    old_fval: float, old value of function\n",
    "    alpha: float, constant coefficient\n",
    "    eta0: float, int, initial step length\n",
    "    eta_min: float, int minimum value for step length\n",
    "    \"\"\"\n",
    "#     xk = np.atleast_1d(xk)\n",
    "    fc = [0]\n",
    "\n",
    "    def phi(eta1):\n",
    "        fc[0] += 1\n",
    "        return f(w + eta1*pk, *args)\n",
    "\n",
    "    if old_fval is None:\n",
    "        fval0 = phi(0.)\n",
    "    else:\n",
    "        fval0 = old_fval  # compute f(xk) -- done in past loop\n",
    "    \n",
    "    # Calcul de la pente\n",
    "    pente = np.dot(pk, gfk)\n",
    "    \n",
    "    eta, phi1 = ls_armijo(w=w, f=f, fval0=fval0,pk=pk, pente=pente, args=args, alpha=alpha,\n",
    "                            eta0=eta0)\n",
    "    return alpha, phi1\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def stable_logistic(x):\n",
    "    \"\"\"Logistic function (for a scalar valued argument).\n",
    "\n",
    "    A numerically stable implementation.\n",
    "    \"\"\"\n",
    "    if x > 0:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x)/(1 + np.exp(x))\n",
    "\n",
    "stable_logistic = np.vectorize(stable_logistic)    \n",
    "\n",
    "def logistic_surrogate_loss(w, X, y):\n",
    "    # Computing the dot product\n",
    "    n, d = X.shape\n",
    "#     print(n)\n",
    "#     print(d)\n",
    "    ps = np.dot(X, w[:-1]) + w[-1]\n",
    "#     print(ps)\n",
    "    yps = y * ps\n",
    "    loss = np.where(yps > 0,\n",
    "                   np.log(1 + np.exp(-yps)),\n",
    "                   (-yps + np.log(1 + np.exp(yps))))\n",
    "    loss = loss.sum() / n\n",
    "    return loss\n",
    "\n",
    "def gradient_log_surrogate_loss(w, X, y):\n",
    "    # defining dim variables\n",
    "    n, d = X.shape\n",
    "    z = X.dot(w[:-1]) + w[-1]\n",
    "    z = stable_logistic(y * z)\n",
    "    z0 = (z - 1) * y\n",
    "    \n",
    "    # initiating g: gradient vector\n",
    "    g = np.zeros(d+1)\n",
    "    # Computing dot product\n",
    "#     ps = (np.dot(X, w[1:])) + w[0]\n",
    "    g[:-1] = X.T.dot(z0)\n",
    "    g[-1] = z0.sum()\n",
    "    g /= n\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=10)\n",
    "w = np.random.random(X.shape[1]+1)\n",
    "loss = logistic_surrogate_loss(w, X, y)\n",
    "g = gradient_log_surrogate_loss(w, X, y)\n",
    "p = -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8791918395941279"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6535967361921888\n",
      "returning eta0 at line 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0001, 0.6535967361921888)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_search(w, f=logistic_surrogate_loss, args=(X, y), pk=p, gfk=g, old_fval=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " 1,\n",
       " 1,\n",
       " 0.6535967361921888,\n",
       " 0.8791918395941279,\n",
       " array([ 0.04763232,  0.06916227,  0.04938514,  0.05991375, -0.19110761,\n",
       "        -0.01601373,  0.12528538, -0.0107462 ,  0.03594911,  0.00514419,\n",
       "        -0.20247957]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import line_search as spls\n",
    "spls(f=logistic_surrogate_loss, myfprime=gradient_log_surrogate_loss, xk = w, old_fval=loss, pk=p, args=(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
