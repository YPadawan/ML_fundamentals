{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48967575999527724"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X, w).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La régression logistique\n",
    "\n",
    "- Méthode introduite par les statisticiens vers la fin des années 60 et popularisée par Andersen (1982).\n",
    "- Permet de s'affranchir des hypothèses restrictives associées aux méthode linéaires paramétriques.\n",
    "- Hypothèse:\n",
    "    - Logarithme des rapports de probabilités conditionnelles des classes pour une entrée $x$ est linéaire par rapport à $x$.\n",
    "\n",
    "$$\\ln{\\Big(\\frac{\\mathbb{P}(X =x | Y = 1)}{\\mathbb{P}(X = x | Y = -1)}\\Big)} = w_0 + \\langle \\bar{w}, x \\rangle$$\n",
    "\n",
    "\n",
    "- La probabilité à posteriori:\n",
    "\n",
    "$$\\mathbb{P}(Y = 1 | X = x) = \\frac{1}{1 + e^{-(\\tilde{w} + \\langle \\bar{w}, x \\rangle)}}$$\n",
    "\n",
    "Ci-dessous se trouve la fonction permettant de calculer l'équation ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le x correspond à formule linéaire de x\n",
    "def logistic(x):\n",
    "    return (1.0 / (1.0 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57074539, 0.62370118, 0.71137554, 0.51537411, 0.63786232,\n",
       "       0.69580472, 0.71342388, 0.56557106, 0.72657534, 0.72220461])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lien avec le principe MRE\n",
    "## Gradient de la fonction de coût\n",
    "\n",
    "### Fonction de coût:\n",
    "\n",
    "$$ \\hat{\\mathcal{L}}(\\textbf{w}) = \\frac{1}{m}\\sum_{t=1}^{m}\\ln{(1 + e^{-yh_w(x_i)})}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_surrogate_loss(X, y, w):\n",
    "    \"\"\" Calcul de la fonction de coût logistique\n",
    "    Paramètres\n",
    "    -----------\n",
    "    X: matrix, or sparse array shape (n, d)\n",
    "    y: array, shape (n,)\n",
    "        True labels\n",
    "    w: array, shape (d+1,)\n",
    "        Weight vectors (the +1 is for the intercept)\n",
    "    Renvoie\n",
    "    -------\n",
    "    loss : float,\n",
    "        Valeur de la fonction de coût\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    S = 0.\n",
    "    ps = 0.\n",
    "    ps += np.dot(X,w[:d]).sum()\n",
    "    S += (logistic(y*ps)).sum() / n\n",
    "    return S\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7395516942151681"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test rapide pour vérifier que logistic_surrogate_loss renvoie toujours\n",
    "# des valeurs entre 0 et 1\n",
    "w0 = 2\n",
    "w = np.random.random(d)\n",
    "logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le gradient de la fonction:\n",
    "\n",
    "$$ \\nabla \\hat{\\mathcal{L}}(\\textbf{w}) = \\frac{1}{m}\\sum_{t=1}^{m}y_i \\Big(1 - \\frac{1}{1 + e^{-y_ih_w(x_i)}}\\Big) \\times x_i $$\n",
    "\n",
    "- Pour l'apprentissage des paramètres du modèle de la régression logistique en utilisant la méthode du gradient conjugué  pour minimser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_logistic_surrogate_loss(X, y, w, w0):\n",
    "#     \"\"\"Calcul du vecteur gradient Eq. (3.17) avec le biais en plus\n",
    "#     Paramètres\n",
    "#     -----------\n",
    "#     X: matrix, or sparse array shape (n, d)\n",
    "#     y: array, shape (n,)\n",
    "#         True labels\n",
    "#     w: array, shape (d,)\n",
    "#         Weight vectors\n",
    "#     w0: scalar,\n",
    "#         bais\n",
    "#     Renvoie\n",
    "#     -------\n",
    "#     grad : array, shape (d,)\n",
    "#         Vecteur gradient de la fonction de coût logistique\n",
    "#     \"\"\"\n",
    "#     n, d = X.shape\n",
    "#     S = 0.\n",
    "#     g = np.zeros(d)\n",
    "#     ps = w0\n",
    "#     ps += np.dot(X,w).sum()\n",
    "#     g0 = 0.\n",
    "#     for i in range(n):\n",
    "#         g0 += ((logistic(y*ps) - 1.0) * y).sum()\n",
    "#         g += np.dot((logistic(y*ps) - 1.0) * y, X)\n",
    "    \n",
    "#     g0 /= n\n",
    "#     g /= n\n",
    "#     return g0, g\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_logistic_surrogate_loss(X, y, w):\n",
    "    \"\"\"Calcul du vecteur gradient Eq. (3.17) avec le biais en plus\n",
    "    Paramètres\n",
    "    -----------\n",
    "    X: matrix, or sparse array shape (n, d)\n",
    "    y: array, shape (n,)\n",
    "        True labels\n",
    "    w: array, shape (d,)\n",
    "        Weight vectors\n",
    "    w0: scalar,\n",
    "        bais\n",
    "    Renvoie\n",
    "    -------\n",
    "    grad : array, shape (d,)\n",
    "        Vecteur gradient de la fonction de coût logistique\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    S = 0.\n",
    "    g = np.zeros(d + 1)\n",
    "    ps = 0.\n",
    "    ps += np.dot(X,w[:d]).sum()\n",
    "    g[-1] = 0.\n",
    "    for i in range(n):\n",
    "        g[-1] += ((logistic(y*ps) - 1.0) * y).sum()\n",
    "        g[:d] += np.dot((logistic(y*ps) - 1.0) * y, X)\n",
    "    \n",
    "    g /= n\n",
    "    return g\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55666761, 0.29095521, 0.51386501, 0.2074061 , 0.39348376,\n",
       "       0.13664427, 0.08507811, 0.26945879, 0.99031271, 0.79390901])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.58451259,   0.90635072,  -3.47033508,  -1.51085909,\n",
       "       -18.95148409,   9.32889442, -47.67674831,  -8.23998802,\n",
       "         3.93501973, -51.05667478, -50.72887508])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = 2\n",
    "# w = np.random.random(d)\n",
    "gradient_logistic_surrogate_loss(X, y, w, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-50.72887508040241,\n",
       " array([ -0.58451259,   0.90635072,  -3.47033508,  -1.51085909,\n",
       "        -18.95148409,   9.32889442, -47.67674831,  -8.23998802,\n",
       "          3.93501973, -51.05667478]))"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = 2\n",
    "# w = np.random.random(d)\n",
    "gradient_logistic_surrogate_loss(X, y, w, w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random(X.shape[1])\n",
    "Lold = logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=np.dot(w, X.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.26205406,  1.14057942, -0.53074518,  4.18987038,  4.11856083,\n",
       "        2.67550385, -5.08599408, -0.03064195,  0.65792374, -4.54851923,\n",
       "       -0.85454781,  6.56639145, -3.03341765,  2.97723981, -4.2677936 ,\n",
       "        2.44211397,  0.52961696, -2.48590008, -1.83935177,  4.19587456,\n",
       "       -2.03377407,  3.58172484,  6.37703684, -2.65518889,  5.89762513,\n",
       "        1.34868881,  2.86373761,  1.0161757 ,  2.44474357,  3.39327161,\n",
       "        3.25917328, -1.35757692,  2.26013013,  2.47831506, -2.67244356,\n",
       "       -2.85177929, -0.79193923, -4.07653855, -2.80997667,  4.0179495 ,\n",
       "       -0.75532801, -2.93184473,  0.95464543,  1.48646668, -1.97657485,\n",
       "        4.04454057,  0.13696562,  2.23237732, -0.36044482,  2.59804393,\n",
       "       -2.89493192, -4.33170818, -4.3765714 , -1.72942169, -3.25581286,\n",
       "       -1.49579574, -3.40499265,  1.64246975, -2.92648224, -5.39705175,\n",
       "       -2.43158457,  0.44560638, -1.00499537, -2.00486985, -3.87651925,\n",
       "       -0.57072789, -2.3445198 , -4.85778221,  2.06400289, -2.28485245,\n",
       "        0.3406071 , -2.78129775,  1.40738163,  2.09336676, -0.90071311,\n",
       "        4.62115875,  2.15237669,  0.85290179, -0.5330059 , -3.20272647,\n",
       "       -1.06136606,  0.35605752,  2.2269602 ,  1.35468636, -1.10124511,\n",
       "        4.02255754,  0.06692171,  2.09138167, -2.91471175, -0.36922538,\n",
       "       -0.13451719,  0.15969003, -4.27724175,  2.35119179,  0.05143602,\n",
       "        1.28287158,  0.96428147, -0.63309143,  4.4378723 ,  0.04339279])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.629564156962264"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1.0 + np.exp(-y*ps)).sum() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64970466, 0.43843095, 0.44007038, 0.67150481, 0.97377696,\n",
       "       0.06883018, 0.42806821, 0.63469849, 0.74762734, 0.7644469 ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09013458, 0.13976353, 0.53514195, 0.23298156, 2.92240777,\n",
       "       1.43855929, 7.3519783 , 1.27064482, 0.60679851, 7.87317882])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.empty_like(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Line search\n",
    "\n",
    "Before coding the conjugate gradient descent we have to code the line search which is used to optimize the learning step $\\eta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(11)\n",
    "b = np.arange(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) (10,)\n"
     ]
    }
   ],
   "source": [
    "print(p.T.shape, g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32249876950429013"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p @ g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-39.11315285010966"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_g = np.empty(10+1)\n",
    "test_p = np.empty(10+1)\n",
    "test_g[:10] = g\n",
    "test_g[-1] = g0\n",
    "test_p[:10] = p\n",
    "test_p[-1] = p0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00450673  0.00698818 -0.0267571  -0.01164908 -0.14612039  0.07192796\n",
      " -0.36759892 -0.06353224  0.03033993 -0.39365894] -39.11315285010966\n",
      "[-4.50672919e-03  6.98817673e-03 -2.67570976e-02 -1.16490780e-02\n",
      " -1.46120389e-01  7.19279647e-02 -3.67598915e-01 -6.35322409e-02\n",
      "  3.03399253e-02 -3.93658941e-01 -3.91131529e+01]\n"
     ]
    }
   ],
   "source": [
    "print(p, p0)\n",
    "print(test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530.1612246455456"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_g @ test_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09013458,  0.13976353, -0.53514195, -0.23298156, -2.92240777,\n",
       "        1.43855929, -7.3519783 , -1.27064482,  0.60679851, -7.87317882])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(20, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09013458,  0.13976353, -0.53514195, -0.23298156, -2.92240777,\n",
       "        1.43855929, -7.3519783 , -1.27064482,  0.60679851, -7.87317882])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20 * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1530.1612246455456"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p @ g + (p0*g0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search(X, y, w, wold, cost_func, p, g, oldloss, newloss):\n",
    "    \n",
    "    alpha = 1e-7\n",
    "    mineta = 1e-4\n",
    "    \n",
    "    n, d = X.shape\n",
    "    # initialisation de wold\n",
    "    wold = w\n",
    "#     oldloss = cost_func(X, y, w)\n",
    "#     newloss = oldloss\n",
    "    \n",
    "    # Calcul de la pente au point actuel (float)\n",
    "    pente = p @ g\n",
    "    \n",
    "    # Définition de la valeur minimale tolérée de eta\n",
    "    _max = 0.\n",
    "#         if np.abs(p[j]) > _max  * np.maximum(np.abs(w[0]), 1):\n",
    "#             _max = np.maximum(np.abs(w[j]), 1.0)\n",
    "#     etamin = _mineta / _max\n",
    "    for j in range(d+1):\n",
    "        if np.abs(p[j]) > _max * np.maximum(np.abs(wold[j]), 1.):\n",
    "            print(np.abs(p[j]))\n",
    "            _max = np.abs(p[j]) / np.maximum(np.abs(wold[j]), 1.)\n",
    "            print(_max)\n",
    "    etamin = mineta / _max\n",
    "    \n",
    "    # Initialisation de eta à 1\n",
    "    eta = 1.\n",
    "    w = wold + eta*p\n",
    "    newloss = cost_func(X, y, w)\n",
    "    \n",
    "    # Boucler tant que la condition d'Armijo n'est pas complète (Eq. 2.18)\n",
    "    while newloss > (oldloss + alpha * eta * pente):\n",
    "        if eta < etamin:\n",
    "            w = wold\n",
    "            break\n",
    "        else:\n",
    "            if eta == 1.:\n",
    "                # Minimiseur du polynôme d'interpolation de degré 2 (Eq. 2.32)\n",
    "                etatmp = -pente / 2.*(oldloss - (oldloss*pente))\n",
    "            else:\n",
    "                coeff1 = newloss - oldloss - eta*pente\n",
    "                coeff2 = newloss2 - oldloss - eta2*pente\n",
    "                \n",
    "                # Calcul des coefficients du polynôme d'interpolation de degré 3 (Eq. 2.33)\n",
    "                a = (coeff1/(eta**2) - coeff2/(eta**2)) / (eta - eta2)\n",
    "                b = (-eta2*coeff1/ (eta**2) + eta*coeff2 / (eta2**2)) / (eta - eta2)\n",
    "                if a != 0.:\n",
    "                    delta = (b**2)-3.*a*pente\n",
    "                    if delta >= 0.:\n",
    "                        # minimiseur du polynôme d'interpolation de degré 3 (Eq 2.34)\n",
    "                        etatmp = (-b + np.sqrt(delta)) / (3.0*a)\n",
    "                    else:\n",
    "#                         print(\"problème d'interpolation\")\n",
    "                        raise ValueError(\"rchln :problème d'interpolation\")\n",
    "                        \n",
    "                else:\n",
    "                    etatmp = -pente / (2. * b)\n",
    "                    if etatmp > 0.5 * eta:\n",
    "                        etatmp = 0.5 * eta\n",
    "        eta2 = eta\n",
    "        newloss2 = newloss\n",
    "        eta = np.maximum(etatmp, 0.1*eta)\n",
    "        w = wold + eta*p\n",
    "        newloss = cost_func(X, y, w)\n",
    "    return newloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO: A quelle valeurs sont initialisées eta et eta2 ?\n",
    "# def line_search(X, y, w, cost_func, grd, wnew, Lold, g, g0, p, p0, L):\n",
    "#     \"\"\"Computing line search algorithm\n",
    "#     Parameters\n",
    "#     -----------\n",
    "#     cost_func : function, \n",
    "#             Cost function to minimize\n",
    "#     grd : function\n",
    "#         Gradient of cost function\n",
    "#     Lold : float,\n",
    "#         Vector of current loss \n",
    "#     g : array \n",
    "#         Vector of gradient\n",
    "#     g0 : float\n",
    "#         Intercept\n",
    "#     p : array \n",
    "#         Vector of Descent direction\n",
    "#     p0 : float\n",
    "#         Intercept (bias) of descent direction\n",
    "#     w : array \n",
    "#         New weight vectors\n",
    "#     L: float \n",
    "#         New loss value\n",
    "#     \"\"\"\n",
    "#     print(L)\n",
    "#     _alpha = 1e-4\n",
    "#     _mineta = 1e-7\n",
    "#     m, d = X.shape\n",
    "#     pente = 0.\n",
    "#     # Computing the value of the slope\n",
    "#     pente = np.sum(p+g)\n",
    "    \n",
    "#     # Defining minimal tolerated value of eta\n",
    "#     _max = 0.\n",
    "#     for j in range(d):\n",
    "#         print(np.abs(p[j]))\n",
    "#         print(np.abs(p[j]) > _max  * np.maximum(np.abs(w[0]), 1))\n",
    "#         if np.abs(p[j]) > _max  * np.maximum(np.abs(w[0]), 1):\n",
    "#             _max = np.maximum(np.abs(w[j]), 1.0)\n",
    "#     etamin = _mineta / _max\n",
    "    \n",
    "#     ## Mise à jour du vecteur poids pour la plus grande valeur de eta\n",
    "#     ## à partir de laquelle on commence la recherche\n",
    "#     eta = 1.0\n",
    "#     wnew = w + (eta*p)\n",
    "    \n",
    "#     L = cost_func(X, y, wnew)\n",
    "    \n",
    "#     # Boucler tant que la condition d'Armijo n'est pas satisfaite (Eq. 2.18)\n",
    "#     while L > (Lold +_alpha*eta*pente):\n",
    "#         if eta < etamin:\n",
    "#             wnew = w\n",
    "#             break\n",
    "#         else:\n",
    "#             if eta == 1.0:\n",
    "#                 etatmp = -pente/(2.0*(L-Lold-pente))\n",
    "#             else:\n",
    "#                 coeff1 = L - Lold - eta*pente\n",
    "#                 coeff2 = L - Lold - eta*pente\n",
    "#                 # Calcul des coefficients du polynôme d'interpolation de degré 3 (Eq 2.33)\n",
    "#                 a = (coeff1/(eta**2) - coeff2/(eta2**2)) / (eta-eta2)\n",
    "#                 if a != 0.:\n",
    "#                     _delta = np.abs(((b**2)-3.0*a*pente).sum())\n",
    "#                     print(\"This is delta!\")\n",
    "#                     print(_delta)\n",
    "#                     if _delta >= 0.:\n",
    "#                         # Le minimiseur du polynôme d'interpolationi de degré 3 (Eq. 2.34)\n",
    "#                         etatmp = ((-b+np.sqrt(_delta))/(3.*a)).sum()\n",
    "#                     else:\n",
    "#                         raise ValueError(\"rchln : problème d'interpolation\")\n",
    "#                 else:\n",
    "#                     etatmp = -pente/(2.0*b)\n",
    "                    \n",
    "#                 if etatmp > 0.5*eta:\n",
    "#                     etatmp = 0.5*eta\n",
    "                    \n",
    "#         eta2 = eta\n",
    "#         L2 = L\n",
    "#         eta = np.maximum(etatmp, 0.1*eta)\n",
    "#         wnew = w + eta*p\n",
    "#         L = cost_func(X, y, wnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65877500552957"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65877500552957\n",
      "0.004506729185637481\n",
      "True\n",
      "0.006988176727331097\n",
      "False\n",
      "0.02675709761357684\n",
      "False\n",
      "0.011649078027182257\n",
      "False\n",
      "0.146120388593539\n",
      "False\n",
      "0.07192796465961238\n",
      "False\n",
      "0.36759891503747\n",
      "False\n",
      "0.06353224085340672\n",
      "False\n",
      "0.030339925340842643\n",
      "False\n",
      "0.3936589410818832\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "w_new = np.random.random(X.shape[1])\n",
    "Lold = logistic_surrogate_loss(X, y, w_new)\n",
    "p0, p = g0, g\n",
    "line_search(X, y, w, logistic_surrogate_loss, g, w, Lold, g, g0, p, p0, Lold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_max = 0.\n",
    "p[0] > _max  * np.maximum(np.abs(w[0]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32249876950429013"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g @ g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.567889751187931"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient conjugué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(X, y, w, cost_func, grd_func, epsilon):\n",
    "    \n",
    "    epoque = 0\n",
    "    n, d = X.shape\n",
    "    wold = np.empty_like(w)\n",
    "    p = np.empty_like(w)\n",
    "    newloss = cost_func(X, y, w)\n",
    "    oldloss = newloss + 2*epsilon\n",
    "    g = grd_func(X, y, w)\n",
    "    # Initialisation de p au gradient en temps 0 (Eq. 2.46)\n",
    "    p = -g\n",
    "    print(p)\n",
    "    \n",
    "    while np.abs(oldloss - newloss) > np.abs(oldloss) * epsilon:\n",
    "        newloss =line_search(X=X, y=y, w=w, wold=wold, cost_func=cost_func, \n",
    "                             p=p, g=g, oldloss=oldloss, newloss=newloss)\n",
    "        print(newloss)\n",
    "        # Calcul du nouveau vecteur gradient (Eq. 2.42)\n",
    "        h = grd_func(X, y, w)\n",
    "        dgg = g @ g\n",
    "        ngg = h @ h\n",
    "        \n",
    "        # Faut-il rajouter la formule de Ribière-Polak (Eq. 2.52)\n",
    "        \n",
    "        # Formule de Fletcher-Reeves (Eq. 2.53)\n",
    "        beta = ngg / dgg\n",
    "        \n",
    "        wold = w\n",
    "        g = h\n",
    "        # Mise à jour de la direction de descente\n",
    "        p = -g + beta * p\n",
    "        \n",
    "        print(f\"Epoque {epoque} and loss is: {newloss}\")\n",
    "        epoque +=1\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15118483 -0.23442863  0.89760604  0.39078539  4.90182252 -2.41292896\n",
      " 12.33164419  2.1312821  -1.01779725 13.20586595 13.12108019]\n",
      "0.1511848335839635\n",
      "0.1511848335839635\n",
      "0.23442862707257114\n",
      "0.23442862707257114\n",
      "0.8976060427128335\n",
      "0.8976060427128335\n",
      "4.901822524224642\n",
      "4.901822524224642\n",
      "12.33164419390876\n",
      "12.33164419390876\n",
      "13.205865949516902\n",
      "13.205865949516902\n",
      "0.755\n",
      "Epoque 0 and loss is: 0.755\n"
     ]
    }
   ],
   "source": [
    "conjugate_gradient(X=X, y=y, w=w, \n",
    "                   cost_func=logistic_surrogate_loss, \n",
    "                   grd_func=gradient_logistic_surrogate_loss, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conjugate_gradient(X, y, w, cost_func, grd_func, eps=0.1):\n",
    "#     # Random initialization of weights\n",
    "#     m, d = X.shape\n",
    "#     w = np.random.random(d)\n",
    "    \n",
    "#     # Initializing losses\n",
    "#     L = cost_func(X, y, w)\n",
    "#     Lold = L + 2*eps\n",
    "#     g, g0 = grd_func(X, y, w)\n",
    "#     p, p0 = -g, -g0\n",
    "#     t = 0\n",
    "# #     line_search(X, y, w, cost_func, g, w, Lold, g, g0, p, p0, L)\n",
    "#     while np.abs(Lold - L) > (np.abs(Lold) * eps):\n",
    "        \n",
    "# #        \n",
    "#         line_search(X, y, w, cost_func, g, w, Lold, g, g0, p, p0, L)\n",
    "#         # line search is supposed to modify the values of w ou initialiser.\n",
    "#         # creating new gradient vector h\n",
    "# #         h = np.gradient(cost_func(X, y, w))\n",
    "#         h, h0 = grd_func(X, y, w)\n",
    "#         dgg = np.linalg.norm(g)\n",
    "#         ngg = np.linalg.norm(h)\n",
    "#         beta = dgg / ngg # Formule de Fletcher-Reeves (Eq 2.53)\n",
    "#         wold = w\n",
    "#         g = h\n",
    "#         h0 = g0\n",
    "#         p = -g + beta*p # MAJ de la direction de descente (Eq 2.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.86723301e-02, 1.30092141e+00, 2.81690443e-01, 1.75550138e+01,\n",
       "       1.69625433e+01, 7.15832083e+00, 2.58673358e+01, 9.38929142e-04,\n",
       "       4.32863644e-01, 2.06890272e+01, 7.30251962e-01, 4.31174967e+01,\n",
       "       9.20162267e+00, 8.86395687e+00, 1.82140622e+01, 5.96392065e+00,\n",
       "       2.80494125e-01, 6.17969920e+00, 3.38321492e+00, 1.76053634e+01,\n",
       "       4.13623696e+00, 1.28287528e+01, 4.06665988e+01, 7.05002805e+00,\n",
       "       3.47819821e+01, 1.81896152e+00, 8.20099310e+00, 1.03261306e+00,\n",
       "       5.97677111e+00, 1.15142922e+01, 1.06222105e+01, 1.84301508e+00,\n",
       "       5.10818820e+00, 6.14204552e+00, 7.14195460e+00, 8.13264511e+00,\n",
       "       6.27167749e-01, 1.66181666e+01, 7.89596888e+00, 1.61439182e+01,\n",
       "       5.70520405e-01, 8.59571354e+00, 9.11347900e-01, 2.20958319e+00,\n",
       "       3.90684813e+00, 1.63583084e+01, 1.87595811e-02, 4.98350851e+00,\n",
       "       1.29920469e-01, 6.74983224e+00, 8.38063082e+00, 1.87636957e+01,\n",
       "       1.91543772e+01, 2.99089939e+00, 1.06003174e+01, 2.23740490e+00,\n",
       "       1.15939749e+01, 2.69770688e+00, 8.56429828e+00, 2.91281676e+01,\n",
       "       5.91260354e+00, 1.98565043e-01, 1.01001569e+00, 4.01950312e+00,\n",
       "       1.50274015e+01, 3.25730326e-01, 5.49677311e+00, 2.35980480e+01,\n",
       "       4.26010794e+00, 5.22055071e+00, 1.16013196e-01, 7.73561715e+00,\n",
       "       1.98072304e+00, 4.38218439e+00, 8.11284103e-01, 2.13551081e+01,\n",
       "       4.63272541e+00, 7.27441467e-01, 2.84095287e-01, 1.02574568e+01,\n",
       "       1.12649792e+00, 1.26776959e-01, 4.95935175e+00, 1.83517513e+00,\n",
       "       1.21274079e+00, 1.61809691e+01, 4.47851532e-03, 4.37387730e+00,\n",
       "       8.49554459e+00, 1.36327383e-01, 1.80948750e-02, 2.55009064e-02,\n",
       "       1.82947970e+01, 5.52810284e+00, 2.64566441e-03, 1.64575950e+00,\n",
       "       9.29838748e-01, 4.00804753e-01, 1.96947105e+01, 1.88293406e-03])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(X, y, cost_func, grd_func, epsilon=0.1):\n",
    "    # Initialisation des poids w\n",
    "    n, d = X.shape\n",
    "    w = np.random.random(d)\n",
    "    loss = cost_func(X, y, w)\n",
    "    old_loss = loss\n",
    "    p0 = grd_func(loss)\n",
    "    t = 0\n",
    "    while np.abs(loss - old_loss) <= epsilon * old_loss:\n",
    "        eta = 1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
