{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/iaamini/Documents/ML_practice/ML_fundamentals/ML_fundamentals/algorithms/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import logistic_regression as lg\n",
    "import optimizers as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, n_features=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random(X.shape[1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.720344839145916"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X, w[:X.shape[1]]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La régression logistique\n",
    "\n",
    "- Méthode introduite par les statisticiens vers la fin des années 60 et popularisée par Andersen (1982).\n",
    "- Permet de s'affranchir des hypothèses restrictives associées aux méthode linéaires paramétriques.\n",
    "- Hypothèse:\n",
    "    - Logarithme des rapports de probabilités conditionnelles des classes pour une entrée $x$ est linéaire par rapport à $x$.\n",
    "\n",
    "$$\\ln{\\Big(\\frac{\\mathbb{P}(X =x | Y = 1)}{\\mathbb{P}(X = x | Y = -1)}\\Big)} = w_0 + \\langle \\bar{w}, x \\rangle$$\n",
    "\n",
    "\n",
    "- La probabilité à posteriori:\n",
    "\n",
    "$$\\mathbb{P}(Y = 1 | X = x) = \\frac{1}{1 + e^{-(\\tilde{w} + \\langle \\bar{w}, x \\rangle)}}$$\n",
    "\n",
    "Ci-dessous se trouve la fonction permettant de calculer l'équation ci-dessus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Le x correspond à formule linéaire de x\n",
    "# def logistic(x):\n",
    "#     return (1.0 / (1.0 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lien avec le principe MRE\n",
    "## Gradient de la fonction de coût\n",
    "\n",
    "### Fonction de coût:\n",
    "\n",
    "$$ \\hat{\\mathcal{L}}(\\textbf{w}) = \\frac{1}{m}\\sum_{t=1}^{m}\\ln{(1 + e^{-yh_w(x_i)})}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def logistic_surrogate_loss(X, y, w):\n",
    "#     \"\"\" Calcul de la fonction de coût logistique\n",
    "#     Paramètres\n",
    "#     -----------\n",
    "#     X: matrix, or sparse array shape (n, d)\n",
    "#     y: array, shape (n,)\n",
    "#         True labels\n",
    "#     w: array, shape (d+1,)\n",
    "#         Weight vectors (the +1 is for the intercept)\n",
    "#     Renvoie\n",
    "#     -------\n",
    "#     loss : float,\n",
    "#         Valeur de la fonction de coût\n",
    "#     \"\"\"\n",
    "#     n, d = X.shape\n",
    "#     S = 0.\n",
    "#     ps = 0.\n",
    "#     ps += np.dot(X,w[:d]).sum()\n",
    "#     S += (logistic(y*ps)).sum() / n\n",
    "#     return S\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test rapide pour vérifier que logistic_surrogate_loss renvoie toujours\n",
    "# des valeurs entre 0 et 1\n",
    "# w0 = 2\n",
    "# w = np.random.random(d)\n",
    "# logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le gradient de la fonction:\n",
    "\n",
    "$$ \\nabla \\hat{\\mathcal{L}}(\\textbf{w}) = \\frac{1}{m}\\sum_{t=1}^{m}y_i \\Big(1 - \\frac{1}{1 + e^{-y_ih_w(x_i)}}\\Big) \\times x_i $$\n",
    "\n",
    "- Pour l'apprentissage des paramètres du modèle de la régression logistique en utilisant la méthode du gradient conjugué  pour minimser "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gradient_logistic_surrogate_loss(X, y, w):\n",
    "#     \"\"\"Calcul du vecteur gradient Eq. (3.17) avec le biais en plus\n",
    "#     Paramètres\n",
    "#     -----------\n",
    "#     X: matrix, or sparse array shape (n, d)\n",
    "#     y: array, shape (n,)\n",
    "#         True labels\n",
    "#     w: array, shape (d+1,)\n",
    "#         Weight vectors\n",
    "#     w0: scalar,\n",
    "#         bais\n",
    "#     Renvoie\n",
    "#     -------\n",
    "#     grad : array, shape (d+1,)\n",
    "#         Vecteur gradient de la fonction de coût logistique\n",
    "#     \"\"\"\n",
    "#     n, d = X.shape\n",
    "#     S = 0.\n",
    "#     g = np.zeros(d + 1)\n",
    "#     ps = 0.\n",
    "#     ps += np.dot(X,w[:d]).sum()\n",
    "#     g[-1] = 0.\n",
    "#     for i in range(n):\n",
    "#         g[-1] += ((logistic(y*ps) - 1.0) * y).sum()\n",
    "#         g[:d] += np.dot((logistic(y*ps) - 1.0) * y, X)\n",
    "    \n",
    "#     g /= n\n",
    "#     return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30797592, 0.30269473, 0.4801337 , 0.4119392 , 0.08254573,\n",
       "       0.55105515, 0.51323093, 0.03623768, 0.063779  , 0.65499226,\n",
       "       0.18500994])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.47377881,   0.12529948,  23.0674621 , -34.90829428,\n",
       "         3.87118441,  -3.7833038 ,  33.01638623,   5.99995905,\n",
       "        -0.47292404,   9.11790205, -32.96146448])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = 2\n",
    "# w = np.random.random(d)\n",
    "lg.gradient_logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.47377881,   0.12529948,  23.0674621 , -34.90829428,\n",
       "         3.87118441,  -3.7833038 ,  33.01638623,   5.99995905,\n",
       "        -0.47292404,   9.11790205, -32.96146448])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0 = 2\n",
    "# w = np.random.random(d)\n",
    "lg.gradient_logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.random(X.shape[1]+1)\n",
    "Lold = lg.logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, d = X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage des paramètres du modèle\n",
    "\n",
    "La régression linéaire utilise la méthode du gradient conjugué afin de minimiser la fonction de coût.\n",
    "\n",
    "Le gradient conjugué utilise lui-même une recherche linéaire afin de trouver le pas d'apprentissage optimal.\n",
    "Un brève présentation de l'algorithme de recherche linéaire s'impose donc.\n",
    "\n",
    "## recherche linéaire\n",
    "\n",
    "On cherche une valeur maximale admissible du pas d'apprentissage en allant selon un direction de descente $p_t$ (vérifiant $p_t^T\\nabla\\mathcal{L}(w)$.\n",
    "On suit toujours un règle de mise à jour avec la condition de décroissance de la fonction de coût.\n",
    "$$\\forall{t} \\in \\mathbb{N}, \\hat{\\mathcal{L}}(w^{(t+1)}) < \\hat{\\mathcal{L}}(w^{(t)}) $$\n",
    "\n",
    "L'algorithme de recherche linéaire consiste à vérifier des conditions que l'on appelle *conditions de Wolfe*.\n",
    "\n",
    "### Condition d'Armijo\n",
    "\n",
    "Permet de répondre aux deux situations où l'équation ci-dessus peut être satisfaite sans pour autant atteindre le minimiseur de $\\mathcal{L}$.\n",
    "\n",
    "$$\\forall{t} \\in \\mathbb{N}, \\hat{\\mathcal{L}}(w^{(t)} + \\eta_tp_t) \\leq \\hat{\\mathcal{L}}(w_t) + \\alpha\\eta_tp_t^{T}\\nabla\\hat{\\mathcal{L}}(w_t)$$\n",
    "\n",
    "la contrainte de décroissance linéaire implique que le taux de décroissance allant $\\hat{\\mathcal{L}}(w^t)$ à $\\hat{\\mathcal{L}}(w^{t+1})$ ne doit pas être plus grand que la descente pondéré par un coefficient alpha.\n",
    "\n",
    "### Condition de courbure\n",
    "\n",
    "Cette condition implique que la descente lors de l'itération suivante soit au moins égale à une fraction $\\beta \\in (\\alpha, 1)$. Donc:\n",
    "$$ \\forall{t} \\in \\mathbb{N}, p^T_t\\nabla\\hat{\\mathcal{L}}(w^{(t)} + \\eta_tp_t) \\geq \\beta p^T_t\\nabla\\hat{\\mathcal{L}}(w^{(t)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = np.arange(10)\n",
    "p = q + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[1] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = gradient_logistic_surrogate_loss(X, y, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 0.2\n",
    "eta2 = 1\n",
    "a = np.array([[1/eta**2, 1/eta2**2], [-eta2/eta, eta/eta2]])\n",
    "z = np.array([1-2-6, 3-4-1]).reshape(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7],\n",
       "       [-2]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-176.99999999999997"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a @ z).reshape(1, -1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = 1/(eta-eta2) * np.dot(a,z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = ab.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = ab[0][0], ab[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TODO:  Harmonize var names\n",
    "# def line_search(X, y, w, cost_func, p, g, old_loss, new_loss, w_old):\n",
    "#     \"\"\"Line search algorithm\n",
    "#     Parameters\n",
    "#     ----------\n",
    "    \n",
    "#     X : array_like, shape (n, d),\n",
    "#         Matrice contenant les données\n",
    "#     y : array, shape (n,)\n",
    "#         Vecteur des labels\n",
    "#     w : array, (d+1,)\n",
    "#         Vecteur des poids avec le biais en plus\n",
    "#     cost_func : callable,\n",
    "#         Fonction de coût\n",
    "#     p : array, (d+1,)\n",
    "#         Vecteur de la direction de descente\n",
    "#     g : array (d+1,)\n",
    "#         Vecteur gradient\n",
    "#     \"\"\"\n",
    "    \n",
    "#     alpha = 1e-4\n",
    "#     mineta = 1e-7\n",
    "    \n",
    "#     n, d = X.shape\n",
    "#     # initialisation de w_old\n",
    "# #     w_old = w.copy()\n",
    "    \n",
    "#     # Initialisation de old_loss et new_loss\n",
    "# #     old_loss = cost_func(X, y, w)\n",
    "#     new_loss = old_loss\n",
    "    \n",
    "#     # Calcul de la pente au point actuel (float)\n",
    "#     pente = p @ g\n",
    "    \n",
    "#     # Définition de la valeur minimale tolérée de eta\n",
    "#     _max = 0.\n",
    "#     for j in range(d):\n",
    "#         if np.abs(p[j]) > _max * np.maximum(np.abs(w_old[j]), 1.):\n",
    "# #             print(np.abs(p[j]))\n",
    "#             _max = np.abs(p[j]) / np.maximum(np.abs(w_old[j]), 1.)\n",
    "# #             print(_max)\n",
    "#     etamin = mineta / _max\n",
    "    \n",
    "#     # Initialisation de eta à 1\n",
    "#     eta = 1.\n",
    "#     w = w_old + eta * p\n",
    "    \n",
    "#     new_loss = cost_func(X, y, w)\n",
    "    \n",
    "#     # Boucler tant que la condition d'Armijo n'est pas complète (Eq. 2.18)\n",
    "#     while new_loss > (old_loss + alpha * eta * pente):\n",
    "#         if eta < etamin:\n",
    "#             w = w_old\n",
    "#             break\n",
    "#         else:\n",
    "#             if eta == 1.:\n",
    "#                 # Minimiseur du polynôme d'interpolation de degré 2 (Eq. 2.32)\n",
    "#                 etatmp = -pente / (2*(new_loss - old_loss - pente))\n",
    "#             else:\n",
    "#                 coeff1 = new_loss - old_loss - eta*pente\n",
    "#                 coeff2 = new_loss2 - old_loss - eta2*pente\n",
    "                \n",
    "#                 d = np.array([[1/(eta**2), -1/(eta2**2)], [-eta2/(eta**2), eta/(eta2**2)]])\n",
    "#                 c = np.array([coeff1, coeff2]).reshape(2, 1)\n",
    "# #                 print(d)\n",
    "# #                 print(c)\n",
    "                \n",
    "#                 ab = (1/(eta-eta2) * np.dot(d,c)).reshape(1, -1)\n",
    "#                 a, b = ab[0][0], ab[0][1]\n",
    "                \n",
    "                \n",
    "#                 # Calcul des coefficients du polynôme d'interpolation de degré 3 (Eq. 2.33)\n",
    "#                 #TODO: Use numpy to compute\n",
    "                \n",
    "# #                 a = (coeff1/(eta**2) - coeff2/(eta**2)) / (eta - eta2)\n",
    "# #                 b = (-eta2*coeff1/ eta**2 + eta*coeff2 / (eta2**2)) / (eta - eta2)\n",
    "#                 if a != 0.:\n",
    "#                     delta = (b**2)-3.*a*pente\n",
    "# #                     print(delta)\n",
    "#                     if delta >= 0.:\n",
    "#                         # minimiseur du polynôme d'interpolation de degré 3 (Eq 2.34)\n",
    "#                         etatmp = (-b + np.sqrt(delta)) / (3.*a)\n",
    "#                     else:\n",
    "# #                         print(\"problème d'interpolation\")\n",
    "#                         raise ValueError(\"rchln:problème d'interpolation\")\n",
    "                        \n",
    "#                 else:\n",
    "#                     etatmp = -pente / (2. * b)\n",
    "#                     if etatmp > 0.5 * eta:\n",
    "#                         etatmp = 0.5 * eta\n",
    "#         eta2 = eta\n",
    "#         new_loss2 = new_loss\n",
    "#         eta = np.maximum(etatmp, 0.1*eta)\n",
    "#         w = w_old + eta*p\n",
    "#         new_loss = cost_func(X, y, w)\n",
    "#     return w, new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.random.random(d+1)\n",
    "w2 = np.random.random(d+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2629939632353619"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.logistic_surrogate_loss(X, y, w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2550829789266817"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lg.logistic_surrogate_loss(X, y, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b48de9d2ce33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_new_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogistic_surrogate_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# w2, test_new_loss = line_search(X, y, w2, cost_func=logistic_surrogate_loss, p=p, g=g)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "w1, test_new_loss = opt.line_search(X, y, w1, cost_func=lg.logistic_surrogate_loss, p=p, g=g)\n",
    "# w2, test_new_loss = line_search(X, y, w2, cost_func=logistic_surrogate_loss, p=p, g=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6470570105885094"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.255"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_surrogate_loss(X, y, w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.255"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_surrogate_loss(X, y, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(11)\n",
    "b = np.arange(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (11,)\n"
     ]
    }
   ],
   "source": [
    "print(ps.T.shape, g.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps @ g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0033798716608000046"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient conjugué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conjugate_gradient(X, y, w, cost_func, grd_func, epsilon):\n",
    "    \n",
    "#     epoque = 0\n",
    "#     n, d = X.shape\n",
    "#     w_old = np.empty_like(w)\n",
    "#     p = np.empty_like(w)\n",
    "    \n",
    "    \n",
    "#     new_loss = cost_func(X, y, w)\n",
    "#     old_loss = new_loss + 2*epsilon\n",
    "#     g = grd_func(X, y, w)\n",
    "#     # Initialisation de p au gradient en temps 0 (Eq. 2.46)\n",
    "#     p = -g\n",
    "#     print(p)\n",
    "    \n",
    "#     while np.abs(old_loss - new_loss) > np.abs(old_loss)*epsilon:\n",
    "#         old_loss = new_loss\n",
    "#         w, new_loss =line_search(X=X, y=y, w=w, cost_func=cost_func, \n",
    "#                                  p=p, g=g, old_loss=old_loss, new_loss=new_loss, w_old=w_old)\n",
    "# #         print(w)\n",
    "#         # Calcul du nouveau vecteur gradient (Eq. 2.42)\n",
    "#         h = grd_func(X, y, w)\n",
    "#         dgg = g @ g\n",
    "#         ngg = h @ h\n",
    "        \n",
    "#         # Faut-il rajouter la formule de Ribière-Polak (Eq. 2.52)\n",
    "        \n",
    "#         # Formule de Fletcher-Reeves (Eq. 2.53)\n",
    "#         beta = ngg / dgg\n",
    "        \n",
    "#         w_old = w\n",
    "#         g = h\n",
    "#         # Mise à jour de la direction de descente\n",
    "#         p = -g + beta * p\n",
    "        \n",
    "#         print(f\"Epoque {epoque} and loss is: {new_loss}\")\n",
    "#         epoque +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.normal(10, 2, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.11551837,  9.04289943,  9.53303687, 11.45298556,  5.0926688 ,\n",
       "        9.89889583, 11.81427155, 10.875974  , 12.44586153,  7.32282205,\n",
       "        6.76311454])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque 0 and loss is: 0.25\n"
     ]
    }
   ],
   "source": [
    "opt.conjugate_gradient(X=X, y=y, w=w, \n",
    "                   cost_func=lg.logistic_surrogate_loss, \n",
    "                   grd_func=lg.gradient_logistic_surrogate_loss, epsilon=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conjugate_gradient(X, y, w, cost_func, grd_func, eps=0.1):\n",
    "#     # Random initialization of weights\n",
    "#     m, d = X.shape\n",
    "#     w = np.random.random(d)\n",
    "    \n",
    "#     # Initializing losses\n",
    "#     L = cost_func(X, y, w)\n",
    "#     Lold = L + 2*eps\n",
    "#     g, g0 = grd_func(X, y, w)\n",
    "#     p, p0 = -g, -g0\n",
    "#     t = 0\n",
    "# #     line_search(X, y, w, cost_func, g, w, Lold, g, g0, p, p0, L)\n",
    "#     while np.abs(Lold - L) > (np.abs(Lold) * eps):\n",
    "        \n",
    "# #        \n",
    "#         line_search(X, y, w, cost_func, g, w, Lold, g, g0, p, p0, L)\n",
    "#         # line search is supposed to modify the values of w ou initialiser.\n",
    "#         # creating new gradient vector h\n",
    "# #         h = np.gradient(cost_func(X, y, w))\n",
    "#         h, h0 = grd_func(X, y, w)\n",
    "#         dgg = np.linalg.norm(g)\n",
    "#         ngg = np.linalg.norm(h)\n",
    "#         beta = dgg / ngg # Formule de Fletcher-Reeves (Eq 2.53)\n",
    "#         wold = w\n",
    "#         g = h\n",
    "#         h0 = g0\n",
    "#         p = -g + beta*p # MAJ de la direction de descente (Eq 2.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.86723301e-02, 1.30092141e+00, 2.81690443e-01, 1.75550138e+01,\n",
       "       1.69625433e+01, 7.15832083e+00, 2.58673358e+01, 9.38929142e-04,\n",
       "       4.32863644e-01, 2.06890272e+01, 7.30251962e-01, 4.31174967e+01,\n",
       "       9.20162267e+00, 8.86395687e+00, 1.82140622e+01, 5.96392065e+00,\n",
       "       2.80494125e-01, 6.17969920e+00, 3.38321492e+00, 1.76053634e+01,\n",
       "       4.13623696e+00, 1.28287528e+01, 4.06665988e+01, 7.05002805e+00,\n",
       "       3.47819821e+01, 1.81896152e+00, 8.20099310e+00, 1.03261306e+00,\n",
       "       5.97677111e+00, 1.15142922e+01, 1.06222105e+01, 1.84301508e+00,\n",
       "       5.10818820e+00, 6.14204552e+00, 7.14195460e+00, 8.13264511e+00,\n",
       "       6.27167749e-01, 1.66181666e+01, 7.89596888e+00, 1.61439182e+01,\n",
       "       5.70520405e-01, 8.59571354e+00, 9.11347900e-01, 2.20958319e+00,\n",
       "       3.90684813e+00, 1.63583084e+01, 1.87595811e-02, 4.98350851e+00,\n",
       "       1.29920469e-01, 6.74983224e+00, 8.38063082e+00, 1.87636957e+01,\n",
       "       1.91543772e+01, 2.99089939e+00, 1.06003174e+01, 2.23740490e+00,\n",
       "       1.15939749e+01, 2.69770688e+00, 8.56429828e+00, 2.91281676e+01,\n",
       "       5.91260354e+00, 1.98565043e-01, 1.01001569e+00, 4.01950312e+00,\n",
       "       1.50274015e+01, 3.25730326e-01, 5.49677311e+00, 2.35980480e+01,\n",
       "       4.26010794e+00, 5.22055071e+00, 1.16013196e-01, 7.73561715e+00,\n",
       "       1.98072304e+00, 4.38218439e+00, 8.11284103e-01, 2.13551081e+01,\n",
       "       4.63272541e+00, 7.27441467e-01, 2.84095287e-01, 1.02574568e+01,\n",
       "       1.12649792e+00, 1.26776959e-01, 4.95935175e+00, 1.83517513e+00,\n",
       "       1.21274079e+00, 1.61809691e+01, 4.47851532e-03, 4.37387730e+00,\n",
       "       8.49554459e+00, 1.36327383e-01, 1.80948750e-02, 2.55009064e-02,\n",
       "       1.82947970e+01, 5.52810284e+00, 2.64566441e-03, 1.64575950e+00,\n",
       "       9.29838748e-01, 4.00804753e-01, 1.96947105e+01, 1.88293406e-03])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test des algos sur un table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "breast_cancer = pd.read_csv('../../DB/breast-cancer-wisconsin.data', sep=',', header=None)\n",
    "breast_cancer.columns = ['col' + str(i) for i in range(len(breast_cancer.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer.drop('col6', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = breast_cancer.iloc[:, 1:-1]\n",
    "y = breast_cancer.iloc[:, -1]\n",
    "y[y==2] = -1\n",
    "y[y==4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, \n",
    "                                                    test_size=.4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque 0 and loss is: 0.5351282124498348\n",
      "Epoque 1 and loss is: 0.26666666666666666\n",
      "Epoque 2 and loss is: 0.26666666666666666\n"
     ]
    }
   ],
   "source": [
    "opt.conjugate_gradient(X=X_train, y=y_train, w=w, \n",
    "                   cost_func=lg.logistic_surrogate_loss, \n",
    "                   grd_func=lg.gradient_logistic_surrogate_loss, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.11551837,  9.04289943,  9.53303687, 11.45298556,  5.0926688 ,\n",
       "        9.89889583, 11.81427155, 10.875974  , 12.44586153,  7.32282205,\n",
       "        6.76311454])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
